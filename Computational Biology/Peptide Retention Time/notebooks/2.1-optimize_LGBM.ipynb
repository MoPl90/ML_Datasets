{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>LightGBM model optimization</h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "To conclude this series of experiments, we will use the optuna package to run a toy hyper-parameter search. The best model will be saved to disk. To this end we will employ a 70:15:15 split of the data for training, validation (early stopping) and a held-out test set to quantify the final performance. The final model will be re-trained on 90% of the data before export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import lightgbm as lgbm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import optuna\n",
    "import mlflow\n",
    "mlflow.autolog()\n",
    "mlflow.set_experiment(\"Peptide retention time regression\")\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.data import load_data, preprocess_data\n",
    "from src.models import LGBMModelHandler\n",
    "from src.util import  rMAE, rMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Optuna trial as a function that selects hyper-parameters and returns the test set rMAE. \n",
    "\n",
    "From hereon, we will make use of the `src.modles.LGBMModelHandler` class for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"../data/Peptides_and_iRT.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    v = trial.suggest_categorical(\"vectorizer\", [0,1])\n",
    "    vectorizer = CountVectorizer if v == 1 else TfidfVectorizer\n",
    "    \n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"l2\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 2, 256),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 51),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 2, 50),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 100),\n",
    "        \"lambda_l1\" : trial.suggest_float(\"lambda_l1\", 0.0, 1.0),\n",
    "        \"lambda_l2\" : trial.suggest_float(\"lambda_l2\", 0.0, 1.0),\n",
    "    }\n",
    "\n",
    "    lgbMH = LGBMModelHandler(model_name=\"LGBM \" + \"_\".join([f\"{k}:{v}\" for k, v in params.items()]) + f\"_{'Countvectorizer' if v == 1 else 'TfidfVectorizer'}\", \n",
    "                             data=data, \n",
    "                             val_frac=0.15,\n",
    "                             test_frac=0.15, \n",
    "                             vectorizer=vectorizer,\n",
    "                             logging_on=False, # MLFlow doesn't like being in a trial\n",
    "                             model_parameters=params)\n",
    "    \n",
    "    lgbMH.train_eval()\n",
    "    \n",
    "    return lgbMH.eval()['rMAE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For illustrative purposes, run 10 trials only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = optuna.samplers.TPESampler(seed=42) \n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run the best trial with more training data, and save it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = study.best_params.pop('vectorizer')\n",
    "\n",
    "vectorizer = CountVectorizer if v == 1 else TfidfVectorizer\n",
    "data = load_data(\"../data/Peptides_and_iRT.tsv\")\n",
    "\n",
    "\n",
    "lgbMH = LGBMModelHandler(model_name=\"LGBM best model\", \n",
    "                            data=data, \n",
    "                            val_frac=0.1, \n",
    "                            vectorizer=vectorizer,\n",
    "                            model_parameters=study.best_params)\n",
    "\n",
    "mlflow.lightgbm.save_model(lgbMH.model, os.path.join(\"..\", \"models\", lgbMH.model_name))\n",
    "\n",
    "print(lgbMH.train_eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now load the checkpoint and re-use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([lgbMH.X_train, lgbMH.X_val])\n",
    "y = pd.concat([lgbMH.y_train, lgbMH.y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = mlflow.lightgbm.load_model(os.path.join(\"..\", \"models\", lgbMH.model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded.score(lgbMH.X_val, lgbMH.y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "y_true = lgbMH.y_train\n",
    "y_pred = loaded.predict(lgbMH.X_train)\n",
    "sns.scatterplot(y_true, y_pred, marker='+', color=\"darkred\")\n",
    "\n",
    "plt.plot([-100,150], [-100, 150], color=\"black\", lw=0.75)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"iRT measured\")\n",
    "plt.ylabel(\"iRT predicted\")\n",
    "\n",
    "\n",
    "plt.title(\"optimized LGBM predictions vs. GT\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "lgbm.plot_importance(loaded, color='darkred', figsize=(8,10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Conclusions </h1>\n",
    "\n",
    "Using LighGBM and some hyper-parameter optimization, we have found a model that achieves a test set rMAE of 3.89. Having used MLflow, we can track all of our experiments, register models, reproduce runs, and revise input and output of each model.\n",
    "\n",
    "Furthermore, the model can be deployed to production immediately, or loaded from a local checkpoint. It also allows model serving and/or dockerization with built in functions (you will need to run `pip install mlflow[extras]`):\n",
    "\n",
    "```> mlflow models serve -m <MODEL> --enable-mlserver```\n",
    "\n",
    "```> mlflow models build -m <MODEL> --enable-mlserver -n <MODEL>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b08f30121e66749dcbc3b1dd36e6e44a5a3035b66bc0d408f96eaf475ebf976d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
